{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Library"
      ],
      "metadata": {
        "id": "sVuQk6tmzAVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltyoioCyiEg",
        "outputId": "78c32ef2-f5e9-4f2d-b3d7-2670da6a4cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in e:\\anaconda\\lib\\site-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in e:\\anaconda\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Ignoring invalid distribution -umpy (e:\\anaconda\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (e:\\anaconda\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -umpy (e:\\anaconda\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "DZxj_ism0Zl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configuration"
      ],
      "metadata": {
        "id": "G8hsF4v5zDlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = r\"\"\n",
        "CONFIG = {\n",
        "    \"client_id\": \"\",\n",
        "    \"client_secret\": \"\",\n",
        "    \"user_agent\": \"\",\n",
        "    \"username\": \"\",\n",
        "    \"password\": \"\",\n",
        "    \"keywords\": [\"tariff\"],\n",
        "    \"subreddits\": [\"all\"],\n",
        "    \"posts_per_batch\": 50,\n",
        "    \"total_posts\": 400,\n",
        "    \"comments_per_post\": 50,\n",
        "    \"batch_dir\": os.path.join(ROOT_DIR, \"batches\"),\n",
        "    \"checkpoint_file\": os.path.join(ROOT_DIR, \"checkpoint.json\"),\n",
        "    \"log_file\": os.path.join(ROOT_DIR, \"harvest.log\")\n",
        "}"
      ],
      "metadata": {
        "id": "qCOCsm_aXYpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Logger Setup"
      ],
      "metadata": {
        "id": "eB8fEAszXeqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(CONFIG[\"batch_dir\"], exist_ok=True)\n",
        "os.makedirs(os.path.dirname(CONFIG[\"log_file\"]), exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    filename=CONFIG[\"log_file\"],\n",
        "    filemode=\"a\",\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger('').addHandler(console)"
      ],
      "metadata": {
        "id": "yh0w9tjqXhlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initialization"
      ],
      "metadata": {
        "id": "4Zs6CyhLXg-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id=CONFIG[\"client_id\"],\n",
        "    client_secret=CONFIG[\"client_secret\"],\n",
        "    user_agent=CONFIG[\"user_agent\"],\n",
        "    username=CONFIG[\"username\"],\n",
        "    password=CONFIG[\"password\"]\n",
        ")"
      ],
      "metadata": {
        "id": "X67LDU9IXoUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reddit.read_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zpYiHqE08HL",
        "outputId": "d54c20ae-9a2f-421f-dd3a-079ae707c95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. State Management"
      ],
      "metadata": {
        "id": "5uwFTB6fXtvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint() -> set:\n",
        "    if os.path.exists(CONFIG[\"checkpoint_file\"]):\n",
        "        with open(CONFIG[\"checkpoint_file\"], \"r\") as f:\n",
        "            return set(json.load(f))\n",
        "    return set()\n",
        "\n",
        "def save_checkpoint(post_ids: set):\n",
        "    with open(CONFIG[\"checkpoint_file\"], \"w\") as f:\n",
        "        json.dump(list(post_ids), f)"
      ],
      "metadata": {
        "id": "rPRf--jyXwm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Comment Filter"
      ],
      "metadata": {
        "id": "DWQMHRofXyvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_comment(c) -> bool:\n",
        "    if not hasattr(c, 'body') or c.body in [\"[removed]\", \"[deleted]\"]:\n",
        "        return False\n",
        "    if not c.author or str(c.author).lower() == \"automoderator\":\n",
        "        return False\n",
        "    body = c.body.lower()\n",
        "    return (\n",
        "        len(body.strip()) > 30 and\n",
        "        'i am a bot' not in body and\n",
        "        'performed automatically' not in body and\n",
        "        'http' not in body\n",
        "    )\n",
        "\n",
        "def extract_comment_data(c) -> dict:\n",
        "    return {\n",
        "        \"comment_id\": c.id,\n",
        "        \"parent_id\": c.parent_id,\n",
        "        \"author\": str(c.author),\n",
        "        \"body\": c.body,\n",
        "        \"score\": c.score,\n",
        "        \"created_utc\": c.created_utc,\n",
        "        \"depth\": c.depth\n",
        "    }\n",
        "\n",
        "def get_filtered_comments(submission, max_comments: int) -> List[dict]:\n",
        "    try:\n",
        "        submission.comments.replace_more(limit=0)\n",
        "        flat_comments = submission.comments.list()\n",
        "        filtered = [extract_comment_data(c) for c in flat_comments if is_valid_comment(c)]\n",
        "        sorted_comments = sorted(filtered, key=lambda x: x[\"score\"], reverse=True)\n",
        "        return sorted_comments[:max_comments]\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"⚠️ Failed to process comments for post {submission.id}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "UxZMWRuEX23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Rate Limit Handling"
      ],
      "metadata": {
        "id": "uWbrWF0jX3jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def throttle(min_delay=1.2, max_delay=1.8):\n",
        "    time.sleep(random.uniform(min_delay, max_delay))"
      ],
      "metadata": {
        "id": "UTFsh-vZX9xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Batch Save"
      ],
      "metadata": {
        "id": "WRjHKWC8X-VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_batch(data, batch_count):\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        batch_file = os.path.join(CONFIG[\"batch_dir\"], f\"batch_{batch_count:03}_{timestamp}.json\")\n",
        "        with open(batch_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        logging.info(f\"✅ Saved batch {batch_count} with {len(data)} posts to {batch_file}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Failed to save batch {batch_count}: {e}\")"
      ],
      "metadata": {
        "id": "blwEi_F1YBZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Monitor Display"
      ],
      "metadata": {
        "id": "gztkXzu0YB84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_monitor(total, batch, last_post, errors):\n",
        "    clear_output(wait=True)\n",
        "    display(f\"✔️ Total Posts Collected: {total}\")\n",
        "    display(f\"📦 Current Batch: {batch}\")\n",
        "    display(f\"🆔 Last Post ID: {last_post}\")\n",
        "    display(f\"⚠️ Errors So Far: {errors}\")"
      ],
      "metadata": {
        "id": "QWXOZBBLYFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Main Harvest Func"
      ],
      "metadata": {
        "id": "LqiX9_viYG4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def harvest_posts():\n",
        "    all_collected = []\n",
        "    total_count = 0\n",
        "    batch_count = 1\n",
        "    error_count = 0\n",
        "    processed_posts = load_checkpoint()\n",
        "\n",
        "    try:\n",
        "        for keyword in CONFIG[\"keywords\"]:\n",
        "            for sub in CONFIG[\"subreddits\"]:\n",
        "                query = reddit.subreddit(sub).search(\n",
        "                    query=keyword,\n",
        "                    sort='top',\n",
        "                    time_filter='year',\n",
        "                    limit=CONFIG[\"total_posts\"]\n",
        "                )\n",
        "\n",
        "                for submission in query:\n",
        "                    if submission.id in processed_posts:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        filtered_comments = get_filtered_comments(submission, CONFIG[\"comments_per_post\"])\n",
        "\n",
        "                        post_data = {\n",
        "                            \"subreddit\": sub,\n",
        "                            \"keyword\": keyword,\n",
        "                            \"post_id\": submission.id,\n",
        "                            \"title\": submission.title or \"\",\n",
        "                            \"selftext\": submission.selftext or \"\",\n",
        "                            \"created_utc\": submission.created_utc,\n",
        "                            \"score\": submission.score,\n",
        "                            \"url\": submission.url or \"\",\n",
        "                            \"comments\": filtered_comments\n",
        "                        }\n",
        "\n",
        "                        all_collected.append(post_data)\n",
        "                        processed_posts.add(submission.id)\n",
        "                        total_count += 1\n",
        "                        logging.info(f\"📄 Collected post {submission.id} ({total_count}) with {len(filtered_comments)} comments\")\n",
        "\n",
        "                        update_monitor(total_count, batch_count, submission.id, error_count)\n",
        "\n",
        "                        if total_count % CONFIG[\"posts_per_batch\"] == 0:\n",
        "                            save_batch(all_collected, batch_count)\n",
        "                            all_collected = []\n",
        "                            batch_count += 1\n",
        "                            save_checkpoint(processed_posts)\n",
        "                            time.sleep(8)\n",
        "\n",
        "                        throttle()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        logging.error(f\"❌ Error processing post {submission.id}: {e}\")\n",
        "                        time.sleep(5)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"🔥 Fatal error: {e}\")\n",
        "        save_checkpoint(processed_posts)\n",
        "\n",
        "    if all_collected:\n",
        "        save_batch(all_collected, batch_count)\n",
        "        save_checkpoint(processed_posts)\n",
        "        logging.info(f\"✅ Final save batch {batch_count} with {len(all_collected)} posts.\")\n",
        "\n",
        "    logging.info(\"🎉 Scraping completed.\")"
      ],
      "metadata": {
        "id": "RcnZbSLVYNRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Test"
      ],
      "metadata": {
        "id": "1pt_rLMqZeyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_post_save():\n",
        "    keyword = CONFIG[\"keywords\"][0]\n",
        "    subreddit = CONFIG[\"subreddits\"][0]\n",
        "\n",
        "    logging.info(\"🚀 Starting single post test\")\n",
        "\n",
        "    submission = next(reddit.subreddit(subreddit).search(query=keyword, sort='top', time_filter='year', limit=1))\n",
        "\n",
        "    filtered_comments = get_filtered_comments(submission, CONFIG[\"comments_per_post\"])\n",
        "\n",
        "    test_data = {\n",
        "        \"subreddit\": subreddit,\n",
        "        \"keyword\": keyword,\n",
        "        \"post_id\": submission.id,\n",
        "        \"title\": submission.title or \"\",\n",
        "        \"selftext\": submission.selftext or \"\",\n",
        "        \"created_utc\": submission.created_utc,\n",
        "        \"score\": submission.score,\n",
        "        \"url\": submission.url or \"\",\n",
        "        \"comments\": filtered_comments\n",
        "    }\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    test_file = os.path.join(CONFIG[\"batch_dir\"], f\"test_post_{timestamp}.json\")\n",
        "\n",
        "    try:\n",
        "        with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump([test_data], f, ensure_ascii=False, indent=2)\n",
        "        logging.info(f\"✅ Test save success: {test_file}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Test save failed: {e}\")"
      ],
      "metadata": {
        "id": "tuMaufHJZgXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_post_save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSr5pz1SZikA",
        "outputId": "f179b62a-85e8-4f21-de2b-3afb7a242779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-09 14:54:36,636 - 🚀 Starting single post test\n",
            "2025-05-09 14:54:41,269 - ✅ Test save success: E:\\\\Users\\\\76044\\\\Desktop\\\\tariff\\batches\\test_post_20250509_145441.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Run"
      ],
      "metadata": {
        "id": "vrDLvGrMYOto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the harvester\n",
        "harvest_posts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Q6EtZoBmYQNO",
        "outputId": "980f4cf8-f027-42bb-f2fd-f51af06bfb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'✔️ Total Posts Collected: 245'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'📦 Current Batch: 5'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'🆔 Last Post ID: 1jo6v9m'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'⚠️ Errors So Far: 0'"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-09 15:15:14,616 - ✅ Saved batch 5 with 45 posts to E:\\\\Users\\\\76044\\\\Desktop\\\\tariff\\batches\\batch_005_20250509_151514.json\n",
            "2025-05-09 15:15:14,617 - ✅ Final save batch 5 with 45 posts.\n",
            "2025-05-09 15:15:14,618 - 🎉 Scraping completed.\n"
          ]
        }
      ]
    }
  ]
}